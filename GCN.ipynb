{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, use_bias=True):\n",
    "        \"\"\"图卷积：L*X*\\theta\n",
    "        Args:\n",
    "        ----------\n",
    "            input_dim: int\n",
    "                节点输入特征的维度\n",
    "            output_dim: int\n",
    "                输出特征维度\n",
    "            use_bias : bool, optional\n",
    "                是否使用偏置\n",
    "        \"\"\"\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.use_bias = use_bias\n",
    "        self.weight = nn.Parameter(torch.Tensor(input_dim, output_dim))\n",
    "        if self.use_bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight)\n",
    "        if self.use_bias:\n",
    "            init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, adjacency, input_feature):\n",
    "        \"\"\"邻接矩阵是稀疏矩阵，因此在计算时使用稀疏矩阵乘法\n",
    "    \n",
    "        Args: \n",
    "        -------\n",
    "            adjacency: torch.sparse.FloatTensor\n",
    "                邻接矩阵\n",
    "            input_feature: torch.Tensor\n",
    "                输入特征\n",
    "        \"\"\"\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        support = torch.mm(input_feature, self.weight.to(device))\n",
    "        output = torch.sparse.mm(adjacency, support)\n",
    "        if self.use_bias:\n",
    "            output += self.bias.to(device)\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "# ## 模型定义\n",
    "class GcnNet(nn.Module):\n",
    "    \"\"\"\n",
    "    定义一个包含两层GraphConvolution的模型\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=343,drop_out=0.1,embed_dim=1024):\n",
    "        super(GcnNet, self).__init__()\n",
    "        self.gcn1 = GraphConvolution(input_dim, embed_dim)\n",
    "        self.gcn2 = GraphConvolution(embed_dim, embed_dim)\n",
    "        self.mlp=nn.Linear(embed_dim,embed_dim)\n",
    "        self.output=nn.Linear(embed_dim,2)\n",
    "        \n",
    "        self.layer_norm=nn.LayerNorm(normalized_shape= embed_dim)\n",
    "    def forward(self, adjacency, feature,drop_out=0.5):\n",
    "        self.dropout = nn.Dropout(p=drop_out)\n",
    "        h1 = F.relu(self.dropout(self.gcn1(adjacency, feature)))\n",
    "        h2 = F.relu(self.dropout(self.gcn2(adjacency, h1)))\n",
    "        h2=self.layer_norm(h2)\n",
    "        h3=torch.sigmoid(self.dropout(self.mlp(h2)))\n",
    "        logits=self.output(h3)\n",
    "        return logits\n",
    "    \n",
    "from sklearn.metrics import roc_auc_score\n",
    "class myGCN:\n",
    "    def __init__(self,print_flag=False,early_stopping=True,seed=55):\n",
    "        self.printFlag=print_flag\n",
    "        self.early_stopping=early_stopping\n",
    "        flag = torch.cuda.is_available()\n",
    "        print(flag)\n",
    "        ngpu= 1\n",
    "        # Decide which device we want to run on\n",
    "        self.device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "        print(self.device)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        pass\n",
    "    def fit(self,adj,features,labels,train_mask,val_mask,test_mask,learning_rate=0.01,epochs=2000,weight=[1.0,2.0],l2_ratio=0.001,\n",
    "                dropout=0.2,min_epoch=1000,max_train_auc=0.99,min_train_auc=0.90,tolerance=200,embed_dim=1024):\n",
    "        weight_decay = l2_ratio\n",
    "        sample_size=features.size()[0]\n",
    "        feature_size=features.size()[1]\n",
    "        # 模型定义：Model, Loss, Optimizer\n",
    "        device = self.device\n",
    "        self.model = GcnNet(input_dim=feature_size,embed_dim=embed_dim).to(device)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(weight,requires_grad=False).to(device))\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.dropout=dropout\n",
    "        self.tensor_x = features.to(device)\n",
    "        self.tensor_y = labels.to(device)\n",
    "        tensor_train_mask = torch.from_numpy(train_mask).to(device)\n",
    "        tensor_test_mask = torch.from_numpy(test_mask).to(device)\n",
    "        tensor_val_mask=torch.from_numpy(val_mask).to(device)\n",
    "        indices = torch.from_numpy(np.asarray([adj.row, adj.col]).astype('int64')).long()\n",
    "        values = torch.from_numpy(adj.data.astype(np.float32))\n",
    "        \n",
    "        self.tensor_adjacency = torch.sparse.FloatTensor(indices, values, (sample_size,sample_size)).to(device)\n",
    "        ###train\n",
    "        train_loss,train_auc,val_auc,test_auc,best_e,best_val_auc=self.train(tensor_train_mask,tensor_val_mask,tensor_test_mask,epochs,min_epoch,max_train_auc,min_train_auc,tolerance)\n",
    "        return train_loss,train_auc,val_auc,test_auc,best_e,best_val_auc\n",
    "    def train(self,tensor_train_mask,tensor_val_mask,tensor_test_mask,epochs,min_epoch=1000,max_train_auc=0.98,min_train_auc=0.95,tolerance=200):\n",
    "        loss_history = []\n",
    "        val_auc_history = []\n",
    "        train_auc_history=[]\n",
    "        test_auc_history=[]\n",
    "        self.model.train()\n",
    "        train_y = self.tensor_y[tensor_train_mask]\n",
    "        stop_increasing=0\n",
    "        max_auc=0\n",
    "        train_auc_min_id=0\n",
    "        for epoch in range(epochs):\n",
    "            logits = self.model(self.tensor_adjacency, self.tensor_x,drop_out=self.dropout)  # 前向传播\n",
    "            train_mask_logits = logits[tensor_train_mask]   # 只选择训练节点进行监督\n",
    "            loss = self.criterion(train_mask_logits, train_y)    # 计算损失值\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()     # 反向传播计算参数的梯度\n",
    "            self.optimizer.step()    # 使用优化方法进行梯度更新\n",
    "            train_acc, _, _ = self.test(tensor_train_mask,drop_out=self.dropout)     # 计算当前模型训练集上的准确率\n",
    "            val_acc, _, _ = self.test(tensor_val_mask,drop_out=0.0)     # 计算当前模型在验证集上的准确率\n",
    "            test_acc, _, _ = self.test(tensor_test_mask,drop_out=0.0) \n",
    "            # 记录训练过程中损失值和准确率的变化，用于画图\n",
    "            loss_history.append(loss.item())\n",
    "            val_auc_history.append(val_acc.item())\n",
    "            train_auc_history.append(train_acc.item())\n",
    "            test_auc_history.append(test_acc.item())\n",
    "            if train_acc>min_train_auc and train_auc_min_id==0:\n",
    "                train_auc_min_id=epoch\n",
    "                print('epoch {} meet the min train auc requirement'.format(epoch))\n",
    "            if train_acc>=max_train_auc:\n",
    "                print('train auc meets the setting line:{}'.format(max_train_auc))\n",
    "                break\n",
    "            if epoch>min_epoch and val_acc.item()<max_auc:\n",
    "                stop_increasing+=1\n",
    "            else:\n",
    "                max_auc=val_acc.item()\n",
    "                stop_increasing=0\n",
    "            if self.early_stopping:\n",
    "                if stop_increasing>=tolerance:\n",
    "                    if self.printFlag:\n",
    "                        print('验证集收敛:epoch {}'.format(epoch))\n",
    "                    break\n",
    "            if epoch%100==0 and self.printFlag:\n",
    "                print(\"Epoch {:03d}: Loss {:.4f}, TrainAuc {:.4}, ValAuc {:.4f}\".format(\n",
    "                    epoch, loss.item(), train_acc.item(), val_acc.item()))\n",
    "        if self.early_stopping==True:\n",
    "            best_e=np.argmax(val_auc_history[min_epoch:])+min_epoch+1\n",
    "            best_val_auc=max(val_auc_history[min_epoch:])\n",
    "        else:\n",
    "            best_e=np.argmax(val_auc_history[train_auc_min_id:])+train_auc_min_id+1\n",
    "            best_val_auc=max(val_auc_history[train_auc_min_id:])\n",
    "        return loss_history,train_auc_history, val_auc_history,test_auc_history,best_e,best_val_auc\n",
    "# 测试函数\n",
    "    def test(self,mask,drop_out=0.1):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(self.tensor_adjacency, self.tensor_x,drop_out=drop_out)\n",
    "            test_mask_logits = torch.softmax(logits[mask],dim=1)\n",
    "            predict_y = test_mask_logits[:,1]\n",
    "            auc = roc_auc_score(self.tensor_y[mask].cpu().numpy(),predict_y.cpu().numpy())\n",
    "        return auc, test_mask_logits.cpu().numpy(), self.tensor_y[mask].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read kge\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "node_embedding=np.load('./data_preprocessing/dglke_result/ComplEx_KG5_3/KG5_ComplEx_entity.npy')\n",
    "node_dict=pd.read_csv('./data_preprocessing/dglke_dataset/KG5/entities.tsv',header=None,sep='\\t',quoting=csv.QUOTE_NONE,index_col=1).to_dict()[0]\n",
    "data=pd.read_csv('./data_preprocessing/RT_SC_B_features_v2.csv')\n",
    "from tqdm import tqdm\n",
    "embed_dim=node_embedding.shape[1]\n",
    "new_col=list(range(0,embed_dim))\n",
    "for i in tqdm(data.index):\n",
    "    node=str(data.loc[i,'year'])+'.comp.'+ str(data.loc[i,'Stkcd'])\n",
    "    data.loc[i,new_col]=node_embedding[node_dict[node],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read adj\n",
    "import scipy.sparse as sp\n",
    "\n",
    "rpt=sp.load_npz('./data_preprocessing/comp_relations/rpt.npz')\n",
    "sc=sp.load_npz('./data_preprocessing/comp_relations/sc.npz')\n",
    "sdse=sp.load_npz('./data_preprocessing/comp_relations/sameManager.npz')\n",
    "sdse2=sp.load_npz('./data_preprocessing/comp_relations/sdse_sameyear.npz')\n",
    "adj_matrix=sdse\n",
    "#adj_matrix2=sp.load_npz('./data_preprocessing/RT_SC_B_v3.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading {} dataset...\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import math\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    \n",
    "    return np.array(mask, dtype=bool)\n",
    "\n",
    "def normalize_adj(adjacency):\n",
    "  adjacency += 0.0001*sp.eye(adjacency.shape[0])\n",
    "  degree = np.array(adjacency.sum(1))\n",
    "  d_hat = sp.diags(np.power(degree, -0.5).flatten())\n",
    "  return (d_hat.dot(adjacency).dot(d_hat)+sp.eye(adjacency.shape[0])).tocoo()\n",
    "def normalize_features(features):\n",
    "  return features / features.sum(1)\n",
    "def load_data(X,y,adj_matrix,train_ratio=0.7,test_ratio=0.1):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...')\n",
    "    idx_features_labels = X.copy()\n",
    "\n",
    "    features = sp.csr_matrix(idx_features_labels, dtype=np.float32)\n",
    "    labels =y.copy()\n",
    "\n",
    "    # build graph\n",
    "    \n",
    "    adj = sp.coo_matrix(adj_matrix, dtype=np.float32)\n",
    "\n",
    "\n",
    "    #features = normalize_features(features)\n",
    "    adj = normalize_adj(adj)\n",
    "\n",
    "\n",
    "    features = torch.FloatTensor(features.todense())\n",
    "    labels = torch.LongTensor(labels)\n",
    "    print('finished')\n",
    "    return adj, features, labels\n",
    "\n",
    "data=pd.read_csv('./data_preprocessing/RT_SC_B_features_v2.csv')\n",
    "financial_featrues=data\n",
    "X=financial_featrues.drop(['year','Stkcd','IndustryCode','label','EquityNatureID'],axis=1)\n",
    "for column in list(X.columns[X.isna().sum() > 0]):\n",
    "    mean_val = X[column].mean()\n",
    "    X[column].fillna(mean_val, inplace=True)\n",
    "\n",
    "#adj_matrix=sp.load_npz('./data_preprocessing/RT_SC_B_v3.npz')\n",
    "X=X.values\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "X=minmax_scale(X)\n",
    "y=financial_featrues['label'].values\n",
    "adj, features, labels=load_data(X,y,adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###tuning\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "node_num=len(features)\n",
    "parmas=[]\n",
    "dim=[1000]\n",
    "l2=[0.01]\n",
    "weight=[[1.0,2.0],[1.0,4.0],[1.0,8.0]]\n",
    "dropout=[0.2,0.4,0.6]\n",
    "iter=itertools.product(dim,weight,l2,dropout)\n",
    "train_ids,test_ids=train_test_split(np.array(range(len(features))),stratify=financial_featrues['label'].values,random_state=5,test_size=0.2)\n",
    "train_mask_row=sample_mask(train_ids,node_num)\n",
    "train_ids,val_ids=train_test_split(train_ids,test_size=0.25,stratify=financial_featrues['label'].values[train_ids],random_state=5)\n",
    "test_mask = sample_mask(test_ids, node_num)\n",
    "train_mask = sample_mask(train_ids, node_num)\n",
    "val_mask = sample_mask(val_ids, node_num)\n",
    "test_mask = sample_mask(test_ids, node_num)\n",
    "val_auc_tmp=[]\n",
    "re=[]\n",
    "for p in iter:\n",
    "    parmas.append(p)\n",
    "    print(p)\n",
    "    gcn=myGCN(print_flag=False,early_stopping=True)\n",
    "    loss_history,train_auc_history,val_auc_history,test_auc_history,best_e,best_val_auc=gcn.fit(adj,features,labels,train_mask,val_mask,test_mask,learning_rate=0.0001,epochs=4000,\n",
    "        weight=p[1],l2_ratio=p[2],dropout=p[3],tolerance=500,min_epoch=300,embed_dim=p[0])\n",
    "    re.append([best_e,best_val_auc,test_auc_history[best_e]])\n",
    "    print('best epoch {}, val auc:{}'.format(best_e,best_val_auc))\n",
    "best_id=np.argmax([i[1] for i in re])\n",
    "print('best params{},best epoch:{},best val auc:{},test auc:{}'.format(parmas[best_id],re[best_id][0],re[best_id][1],re[best_id][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===previous year,RT_SC_B_V2\n",
    "\n",
    "\n",
    "**best params(0.0001, [1.0, 6.0], 0.01, 0.5),best epoch:1415,best val auc:0.7538483692130463,test auc:0.751702931921099**\n",
    "\n",
    "new npz:\n",
    "best params(0.0001, [1.0, 2.0], 0.01, 0.4),best epoch:1804,best val auc:0.749967600259198,test auc:0.738921348530202\n",
    "emd dim=1500,best params(0.0001, [1.0, 8.0], 0.01, 0.2),best epoch:848,best val auc:0.7751529987760097,test auc:0.7724451224192186\n",
    "\n",
    "---\n",
    "\n",
    "===RT_SC_B_V2_withkg4\n",
    "\n",
    "**best params(0.0001, [1.0, 4.0], 0.001, 0.3),best epoch:1655,best val auc:0.758,test auc:0.755**\n",
    "\n",
    "new npz:\n",
    "params[0.0001, [1.0, 6.0], 0.01, 0.5, 0.97, 0.95], epoch:869,best val auc:0.7522139822881416,test auc:0.7415213178294574\n",
    "KG4_1:params[0.0001, [1.0, 6.0], 0.001, 0.3, 0.99, 0.9], epoch:587,best val auc:0.7687930496556028,test auc:0.7757550464348761\n",
    "\n",
    "KG4_3:embed dim=2000,best params(0.0001, [1.0, 8.0], 0.01, 0.4),best epoch:622,best val auc:0.7797801617587059,test auc:0.7638441169698366\n",
    "\n",
    "===RPT+SC\n",
    "best params(0.0001, [1.0, 2.0], 0.01, 0.6),best epoch:837,best val auc:0.7237538099695203,test auc:0.7222733901297106\n",
    "f1:0.32980972515856233,recall:0.3023255813953488,precision:0.3627906976744186\n",
    "\n",
    "===SDSE+SC(same year sdse)\n",
    "best params(0.0001, [1.0, 8.0], 0.01, 0.6),best epoch:1462,best val auc:0.7297525619795042,test auc:0.7048483191342391\n",
    "f1:0.3660287081339713,recall:0.5930232558139535,precision:0.2647058823529412\n",
    "\n",
    "===SDSE+SC(all year sdse)\n",
    "params[0.0001, [1.0, 8.0], 0.01, 0.6, 0.99, 0.5], epoch:1072,best val auc:0.7654642762857896,test auc:0.7636042674034846\n",
    "f1:0.41450777202072536,recall:0.6201550387596899,precision:0.311284046692607\n",
    "\n",
    "#params[0.0001, [1.0, 8.0], 0.2, 0.6, 0.99, 0.5], epoch:889,best val auc:0.772,test auc:0.777429\n",
    "\n",
    "===RPT+SDSE\n",
    "params[0.0001, [1.0, 6.0], 0.01, 0.2, 0.99, 0.9], epoch:842,best val auc:0.7729402164782682,test auc:0.7698187696676645\n",
    "f1:0.4455284552845529,recall:0.5310077519379846,precision:0.38375350140056025\n",
    "\n",
    "===RPT+SDSE+SC+KG\n",
    "complex kg5_3\n",
    "**dim=1000,best params(1000, [1.0, 6.0], 0.01, 0.5),best epoch:1027,best val auc:0.7768857849137206,test auc:0.7760116854708726**\n",
    "f1:0.44791667,recall:0.5,precision:0.40566 (0.55)\n",
    "complex kg5_2\n",
    "best params(1500, [1.0, 6.0], 0.01, 0.5),best epoch:1020,best val auc:0.7799025607795138,test auc:0.7740449190267864\n",
    "\n",
    "===RPT+SDSE+SC\n",
    "**best params(0.0001, [1.0, 2.0], 0.01, 0.2),best epoch:951,best val auc:0.7744354045167638,test auc:0.7731478816486299**\n",
    "f1:0.444,recall:0.43023255813953487,precision:0.45867768595041325\n",
    "\n",
    "===SDSE+KG\n",
    "KG5_3\n",
    "best params(1000, [1.0, 4.0], 0.01, 0.4),best epoch:568,best val auc:0.7716610267117864,test auc:0.7640168086576099\n",
    "\n",
    "===SDSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0001, [1.0, 6.0], 0.01, 0.5, 0.99, 0.9]\n",
      "True\n",
      "cuda:0\n",
      "Epoch 000: Loss 0.7117, TrainAuc 0.5006, ValAuc 0.5606\n",
      "Epoch 100: Loss 0.6803, TrainAuc 0.5953, ValAuc 0.6578\n",
      "Epoch 200: Loss 0.6540, TrainAuc 0.6588, ValAuc 0.6843\n",
      "Epoch 300: Loss 0.5874, TrainAuc 0.7562, ValAuc 0.7405\n",
      "Epoch 400: Loss 0.4864, TrainAuc 0.8497, ValAuc 0.7569\n",
      "epoch 467 meet the min train auc requirement\n",
      "Epoch 500: Loss 0.3912, TrainAuc 0.9125, ValAuc 0.7618\n",
      "Epoch 600: Loss 0.3128, TrainAuc 0.939, ValAuc 0.7648\n",
      "Epoch 700: Loss 0.2716, TrainAuc 0.9563, ValAuc 0.7658\n",
      "Epoch 800: Loss 0.2464, TrainAuc 0.9643, ValAuc 0.7635\n",
      "Epoch 900: Loss 0.2107, TrainAuc 0.9711, ValAuc 0.7636\n",
      "Epoch 1000: Loss 0.1978, TrainAuc 0.9745, ValAuc 0.7676\n",
      " params[0.0001, [1.0, 6.0], 0.01, 0.5, 0.99, 0.9], epoch:830,best val auc:0.7702210382316942,test auc:0.7730351523524446\n"
     ]
    }
   ],
   "source": [
    "#手动调参\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "node_num=len(features)\n",
    "train_ids,test_ids=train_test_split(np.array(range(len(features))),stratify=financial_featrues['label'].values,random_state=5,test_size=0.2)\n",
    "train_mask_row=sample_mask(train_ids,node_num)\n",
    "\n",
    "train_ids,val_ids=train_test_split(train_ids,test_size=0.25,stratify=financial_featrues['label'].values[train_ids],random_state=5)\n",
    "test_mask = sample_mask(test_ids, node_num)\n",
    "train_mask = sample_mask(train_ids, node_num)\n",
    "val_mask = sample_mask(val_ids, node_num)\n",
    "# lr,weight,l2_ratio,dropout\n",
    "p=[0.0001,[1.0,6.0],0.01,0.5,0.99,0.90]\n",
    "print(p)\n",
    "gcn=myGCN(print_flag=True,early_stopping=False)\n",
    "loss_history,train_auc_history,val_auc_history,test_auc_history,best_e,best_val_auc=gcn.fit(adj,features,labels,train_mask,val_mask,test_mask,learning_rate=p[0],epochs=1027,weight=p[1],\n",
    "    l2_ratio=p[2],dropout=p[3],min_epoch=500,max_train_auc=p[4],min_train_auc=p[5],tolerance=500,embed_dim=1000)\n",
    "print(' params{}, epoch:{},best val auc:{},test auc:{}'.format(p,best_e,best_val_auc,test_auc_history[best_e-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:0.4455284552845529,recall:0.5310077519379846,precision:0.38375350140056025\n"
     ]
    }
   ],
   "source": [
    "###get recall、precision、F1\n",
    "auc,logit,y=gcn.test(test_mask,drop_out=0)\n",
    "predict_y=[0 if logit[i,0]>logit[i,1] else 1 for i in range(len(logit))]\n",
    "from sklearn import metrics\n",
    "def Find_Optimal_Cutoff(TPR, FPR, threshold):\n",
    "    y = TPR - FPR\n",
    "    Youden_index = np.argmax(y)  # Only the first occurrence is returned.\n",
    "    optimal_threshold = threshold[Youden_index]\n",
    "    point = [FPR[Youden_index], TPR[Youden_index]]\n",
    "    return optimal_threshold, point\n",
    "\n",
    "\n",
    "y_pred=predict_y\n",
    "f1=metrics.f1_score(y,y_pred)\n",
    "recall=metrics.recall_score(y,y_pred)\n",
    "precision=metrics.precision_score(y,y_pred)\n",
    "print('f1:{},recall:{},precision:{}'.format(f1,recall,precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###plot\n",
    "import matplotlib.pylab as plt\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(range(len(train_auc_history)),train_auc_history,'b-',label='train auc')\n",
    "ax1.plot(range(len(val_auc_history)),val_auc_history,'r-',label='val auc')\n",
    "\n",
    "ax1.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to train a weighted adj for GCN(failed):\n",
    "\n",
    "weights are tend to be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, use_bias=True):\n",
    "        \"\"\"图卷积：L*X*\\theta\n",
    "        Args:\n",
    "        ----------\n",
    "            input_dim: int\n",
    "                节点输入特征的维度\n",
    "            output_dim: int\n",
    "                输出特征维度\n",
    "            use_bias : bool, optional\n",
    "                是否使用偏置\n",
    "        \"\"\"\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.use_bias = use_bias\n",
    "        self.weight = nn.Parameter(torch.Tensor(input_dim, output_dim))\n",
    "        if self.use_bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight)\n",
    "        if self.use_bias:\n",
    "            init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, adjacency,weight, input_feature):\n",
    "        \"\"\"邻接矩阵是稀疏矩阵，因此在计算时使用稀疏矩阵乘法\n",
    "    \n",
    "        Args: \n",
    "        -------\n",
    "            adjacency: torch.sparse.FloatTensor\n",
    "                邻接矩阵\n",
    "            input_feature: torch.Tensor\n",
    "                输入特征\n",
    "        \"\"\"\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        k=0\n",
    "        weight=weight/weight.sum()\n",
    "        for adj in adjacency:\n",
    "            support = torch.mm(input_feature, self.weight.to(device))\n",
    "            output = torch.sparse.mm(adj, support)\n",
    "            if self.use_bias:\n",
    "                output += self.bias.to(device)\n",
    "            if k==0:\n",
    "                real_output=output*weight[k]\n",
    "            else:\n",
    "                real_output+=output*weight[k]\n",
    "\n",
    "        return real_output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "# ## 模型定义\n",
    "class GcnNet2(nn.Module):\n",
    "    \"\"\"\n",
    "    定义一个包含两层GraphConvolution的模型\n",
    "    \"\"\"\n",
    "    def __init__(self, adj_list,input_dim=343,embed_dim=1024):\n",
    "        super(GcnNet2, self).__init__()\n",
    "        self.gcn1 = GraphConvolution(input_dim, embed_dim)\n",
    "        self.gcn2 = GraphConvolution(embed_dim, embed_dim)\n",
    "        self.mlp=nn.Linear(embed_dim,embed_dim)\n",
    "        self.output=nn.Linear(embed_dim,2)\n",
    "        self.adj_list=adj_list\n",
    "        self.adj_weight=nn.Parameter(torch.Tensor(len(adj_list)))\n",
    "        nn.init.constant_(self.adj_weight,1.0)\n",
    "        self.layer_norm=nn.LayerNorm(normalized_shape= embed_dim)\n",
    "    def forward(self, feature,drop_out=0.5):\n",
    "        self.dropout = nn.Dropout(p=drop_out)\n",
    "        h1 = F.relu(self.dropout(self.gcn1(self.adj_list,self.adj_weight, feature)))\n",
    "        h2 = F.relu(self.dropout(self.gcn2(self.adj_list,self.adj_weight, h1)))\n",
    "        h2=self.layer_norm(h2)\n",
    "        h3=torch.sigmoid(self.dropout(self.mlp(h2)))\n",
    "        logits=self.output(h3)\n",
    "        return logits\n",
    "    def get_adj_weight(self):\n",
    "        print(self.adj_weight)\n",
    "        return self.adj_weight\n",
    "from sklearn.metrics import roc_auc_score\n",
    "class myGCN2:\n",
    "    def __init__(self,print_flag=False,early_stopping=True,seed=55):\n",
    "        self.printFlag=print_flag\n",
    "        self.early_stopping=early_stopping\n",
    "        flag = torch.cuda.is_available()\n",
    "        print(flag)\n",
    "        ngpu= 1\n",
    "        # Decide which device we want to run on\n",
    "        self.device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "        print(self.device)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        pass\n",
    "    def fit(self,adj_list,features,labels,train_mask,val_mask,test_mask,learning_rate=0.01,epochs=2000,weight=[1.0,2.0],l2_ratio=0.001,\n",
    "                dropout=0.2,min_epoch=1000,max_train_auc=0.99,min_train_auc=0.90,tolerance=200,embed_dim=1024):\n",
    "        weight_decay = l2_ratio\n",
    "        sample_size=features.size()[0]\n",
    "        feature_size=features.size()[1]\n",
    "        # 模型定义：Model, Loss, Optimizer\n",
    "        device = self.device\n",
    "       \n",
    "        self.dropout=dropout\n",
    "        self.tensor_x = features.to(device)\n",
    "        self.tensor_y = labels.to(device)\n",
    "        tensor_train_mask = torch.from_numpy(train_mask).to(device)\n",
    "        tensor_test_mask = torch.from_numpy(test_mask).to(device)\n",
    "        tensor_val_mask=torch.from_numpy(val_mask).to(device)\n",
    "        self.tensor_adjacency_list=[]\n",
    "        for adj in adj_list:\n",
    "            indices = torch.from_numpy(np.asarray([adj.row, adj.col]).astype('int64')).long()\n",
    "            values = torch.from_numpy(adj.data.astype(np.float32))\n",
    "            \n",
    "            self.tensor_adjacency_list.append(torch.sparse.FloatTensor(indices, values, (sample_size,sample_size)).to(device))\n",
    "        self.model = GcnNet2(self.tensor_adjacency_list,input_dim=feature_size,embed_dim=embed_dim).to(device)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(weight,requires_grad=False).to(device))\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        ###train\n",
    "        train_loss,train_auc,val_auc,test_auc,best_e,best_val_auc=self.train(tensor_train_mask,tensor_val_mask,tensor_test_mask,epochs,min_epoch,max_train_auc,min_train_auc,tolerance)\n",
    "        return train_loss,train_auc,val_auc,test_auc,best_e,best_val_auc\n",
    "    def train(self,tensor_train_mask,tensor_val_mask,tensor_test_mask,epochs,min_epoch=1000,max_train_auc=0.98,min_train_auc=0.95,tolerance=200):\n",
    "        loss_history = []\n",
    "        val_auc_history = []\n",
    "        train_auc_history=[]\n",
    "        test_auc_history=[]\n",
    "        self.model.train()\n",
    "        train_y = self.tensor_y[tensor_train_mask]\n",
    "        stop_increasing=0\n",
    "        max_auc=0\n",
    "        train_auc_min_id=0\n",
    "        for epoch in range(epochs):\n",
    "            logits = self.model( self.tensor_x,drop_out=self.dropout)  # 前向传播\n",
    "            train_mask_logits = logits[tensor_train_mask]   # 只选择训练节点进行监督\n",
    "            loss = self.criterion(train_mask_logits, train_y)    # 计算损失值\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()     # 反向传播计算参数的梯度\n",
    "            self.optimizer.step()    # 使用优化方法进行梯度更新\n",
    "            train_acc, _, _ = self.test(tensor_train_mask,drop_out=self.dropout)     # 计算当前模型训练集上的准确率\n",
    "            val_acc, _, _ = self.test(tensor_val_mask,drop_out=0.0)     # 计算当前模型在验证集上的准确率\n",
    "            test_acc, _, _ = self.test(tensor_test_mask,drop_out=0.0) \n",
    "            # 记录训练过程中损失值和准确率的变化，用于画图\n",
    "            loss_history.append(loss.item())\n",
    "            val_auc_history.append(val_acc.item())\n",
    "            train_auc_history.append(train_acc.item())\n",
    "            test_auc_history.append(test_acc.item())\n",
    "            if train_acc>min_train_auc and train_auc_min_id==0:\n",
    "                train_auc_min_id=epoch\n",
    "                print('epoch {} meet the min train auc requirement'.format(epoch))\n",
    "            if train_acc>=max_train_auc:\n",
    "                print('train auc meets the setting line:{}'.format(max_train_auc))\n",
    "                break\n",
    "            if epoch>min_epoch and val_acc.item()<max_auc:\n",
    "                stop_increasing+=1\n",
    "            else:\n",
    "                max_auc=val_acc.item()\n",
    "                stop_increasing=0\n",
    "            if self.early_stopping:\n",
    "                if stop_increasing>=tolerance:\n",
    "                    if self.printFlag:\n",
    "                        print('验证集收敛:epoch {}'.format(epoch))\n",
    "                    break\n",
    "            if epoch%100==0 and self.printFlag:\n",
    "                print(\"Epoch {:03d}: Loss {:.4f}, TrainAuc {:.4}, ValAuc {:.4f}\".format(\n",
    "                    epoch, loss.item(), train_acc.item(), val_acc.item()))\n",
    "        if self.early_stopping==True:\n",
    "            best_e=np.argmax(val_auc_history[min_epoch:])+min_epoch+1\n",
    "            best_val_auc=max(val_auc_history[min_epoch:])\n",
    "        else:\n",
    "            best_e=np.argmax(val_auc_history[train_auc_min_id:])+train_auc_min_id+1\n",
    "            best_val_auc=max(val_auc_history[train_auc_min_id:])\n",
    "        self.model.get_adj_weight()\n",
    "        return loss_history,train_auc_history, val_auc_history,test_auc_history,best_e,best_val_auc\n",
    "# 测试函数\n",
    "    def test(self,mask,drop_out=0.1):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(self.tensor_x,drop_out=drop_out)\n",
    "            test_mask_logits = torch.softmax(logits[mask],dim=1)\n",
    "            predict_y = test_mask_logits[:,1]\n",
    "            auc = roc_auc_score(self.tensor_y[mask].cpu().numpy(),predict_y.cpu().numpy())\n",
    "        return auc, test_mask_logits.cpu().numpy(), self.tensor_y[mask].cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_list = [normalize_adj(sp.coo_matrix(adj, dtype=np.float32)) for adj in [rpt,sdse,sc]]\n",
    "node_num=len(features)\n",
    "train_ids,test_ids=train_test_split(np.array(range(len(features))),stratify=financial_featrues['label'].values,random_state=5,test_size=0.2)\n",
    "train_mask_row=sample_mask(train_ids,node_num)\n",
    "\n",
    "train_ids,val_ids=train_test_split(train_ids,test_size=0.25,stratify=financial_featrues['label'].values[train_ids],random_state=5)\n",
    "test_mask = sample_mask(test_ids, node_num)\n",
    "train_mask = sample_mask(train_ids, node_num)\n",
    "val_mask = sample_mask(val_ids, node_num)\n",
    "# lr,weight,l2_ratio,dropout\n",
    "p=[0.0001,[1.0,8.0],0.01,0.6,0.99,0.90]\n",
    "print(p)\n",
    "gcn=myGCN2(print_flag=True,early_stopping=True)\n",
    "loss_history,train_auc_history2,val_auc_history2,test_auc_history2,best_e2,best_val_auc2=gcn.fit(adj_list,features,labels,train_mask,val_mask,test_mask,learning_rate=p[0],epochs=3000,weight=p[1],\n",
    "    l2_ratio=p[2],dropout=p[3],min_epoch=500,max_train_auc=p[4],min_train_auc=p[5],tolerance=500,embed_dim=1200)\n",
    "print(' params{}, epoch:{},best val auc:{},test auc:{}'.format(p,best_e,best_val_auc2,test_auc_history2[best_e2]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
