{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,train_test_split\n",
    "import logging\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost\n",
    "from sklearn import metrics\n",
    "class experiment_protocal():\n",
    "    def __init__(self,params_dict,model,exp_name,comment):\n",
    "        self.parmas_dict=params_dict\n",
    "        self.model=model\n",
    "        self.exp_name=exp_name\n",
    "        \n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        self.logger.setLevel(level = logging.INFO)\n",
    "        handler = logging.FileHandler('./log/'+self.exp_name+'.txt')\n",
    "        handler.setLevel(logging.INFO)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        self.logger.addHandler(handler)\n",
    "        self.logger.info(comment)\n",
    "        self.logger.info(\"Start print log\")\n",
    "    def cv_train_test_split(self,X,y,cv_num,seed=5):\n",
    "        kf=KFold(n_splits=cv_num,shuffle=True,random_state=seed)\n",
    "        k=0\n",
    "        return kf.split(X,y)\n",
    "    def select_best_model(self,X_train,y_train,X_val,y_val,seed=5,printFlag=False):\n",
    "        result=[]\n",
    "        for params in self.parmas_dict:\n",
    "            if self.model=='RF':\n",
    "                clf = RandomForestClassifier(n_estimators=params.get('n_estimators',100),random_state=0,class_weight='balanced',max_depth=params.get('max_depth',10),\\\n",
    "                    min_samples_leaf=params.get('min_sample_leaf',1),n_jobs=-1)\n",
    "                \n",
    "                \n",
    "                clf.fit(X_train,y_train)\n",
    "                train_auc=metrics.roc_auc_score(y_train,clf.predict_proba(X_train)[:,1])\n",
    "                y_prob=clf.predict_proba(X_val)\n",
    "                y_prob=y_prob[:,1]\n",
    "                auc=metrics.roc_auc_score(y_val,y_prob)\n",
    "                if printFlag:\n",
    "                    print(params)\n",
    "                    print('val auc:{},train auc:{}'.format(auc,train_auc))\n",
    "                result.append(auc)\n",
    "            elif self.model=='XGBoost':\n",
    "                \n",
    "                #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "                Dtrain=xgboost.DMatrix(X_train,y_train)\n",
    "                Dval=xgboost.DMatrix(X_val,y_val)\n",
    "                model=xgboost.train(params,Dtrain,num_boost_round=2000,early_stopping_rounds=100,evals=[ (Dtrain, 'train'),(Dval, 'eval')],verbose_eval=200)\n",
    "                y_prob=model.predict(Dval)\n",
    "                auc=metrics.roc_auc_score(y_val,y_prob)\n",
    "                result.append(auc)\n",
    "        return self.parmas_dict[np.argmax(result)],max(result)\n",
    "    def run_model(self,X_train,y_train,X_test,y_test,params):\n",
    "        if self.model=='RF':\n",
    "            clf = RandomForestClassifier(n_estimators=params.get('n_estimators',100),random_state=0,class_weight='balanced',max_depth=params.get('max_depth',10),\\\n",
    "                    min_samples_leaf=params.get('min_sample_leaf',1),verbose=200)\n",
    "            clf.fit(X_train,y_train)\n",
    "            y_prob=clf.predict_proba(X_test)\n",
    "            y_prob=y_prob[:,1]\n",
    "            testauc=(metrics.roc_auc_score(y_test,y_prob))\n",
    "            y_prob=clf.predict_proba(X_train)\n",
    "            y_prob=y_prob[:,1]\n",
    "            trainauc=(metrics.roc_auc_score(y_train,y_prob))\n",
    "            return trainauc,testauc\n",
    "        elif self.model=='XGBoost':\n",
    "            Dtrain=xgboost.DMatrix(X_train,y_train)\n",
    "            Dval=xgboost.DMatrix(X_test,y_test)\n",
    "            model=xgboost.train(params,Dtrain,num_boost_round=2000,early_stopping_rounds=100,evals=[ (Dtrain, 'train'),(Dval, 'eval')],verbose_eval=200)\n",
    "            y_prob=model.predict(Dval)\n",
    "            testauc=metrics.roc_auc_score(y_test,y_prob)\n",
    "            y_prob=model.predict(Dtrain)\n",
    "            trainauc=metrics.roc_auc_score(y_train,y_prob)\n",
    "            return trainauc,testauc\n",
    "    def run(self,X,y,cv_num):\n",
    "        split=self.cv_train_test_split(X,y,cv_num)\n",
    "        test_re=[]\n",
    "        val_re=[]\n",
    "        k=0\n",
    "        for train_id,test_id in split:\n",
    "            print('train test split {}'.format(k))\n",
    "            self.logger.info('train test split {}'.format(k))\n",
    "            train_X,test_X,train_y,test_y=X[train_id],X[test_id],y[train_id],y[test_id]\n",
    "            best_params,best_auc=self.select_best_model(train_X,train_y,cv_num=4)\n",
    "            print('best params:{},bets auc on val set:{}'.format(best_params,best_auc))\n",
    "            self.logger.info('best params:{},bets auc on val set:{}'.format(best_params,best_auc))\n",
    "            trainauc,testauc=self.run_model(train_X,train_y,test_X,test_y,best_params)\n",
    "            print('test auc:{}'.format(testauc))\n",
    "            self.logger.info('test auc:{}'.format(testauc))\n",
    "            test_re.append(testauc)\n",
    "            val_re.append(best_auc)\n",
    "            k+=1\n",
    "        self.logger.info('validation set result:{}'.format(val_re))\n",
    "        self.logger.info('test set result:{}'.format(test_re))\n",
    "        logging.shutdown(self.logger)\n",
    "        return test_re,val_re\n",
    "    def simple_run(self,X,y,test_size=0.2,val_zise=0.2,random_state=5):\n",
    "        \"\"\"\n",
    "        return best params, bset validation auc, best test auc\n",
    "        \"\"\"\n",
    "        self.logger.info('simple run with fixed train test split, test size:{}'.format(test_size))\n",
    "        train_X,test_X,train_y,test_y=train_test_split(X,y,stratify=y,test_size=test_size,random_state=random_state)\n",
    "        train_X,val_X,train_y,val_y=train_test_split(train_X,train_y,stratify=train_y,random_state=random_state,test_size=val_zise/(1-test_size))\n",
    "        best_params,best_auc=self.select_best_model(train_X,train_y,val_X,val_y,printFlag=True)\n",
    "        print('best params:{},bets auc on val set:{}'.format(best_params,best_auc))\n",
    "        self.logger.info('best params:{},bets auc on val set:{}'.format(best_params,best_auc))\n",
    "        trainauc,testauc=self.run_model(train_X,train_y,test_X,test_y,best_params)\n",
    "        print('test auc:{}'.format(testauc))\n",
    "        self.logger.info('test auc:{}'.format(testauc))\n",
    "        #logging.shutdown(self.logger)\n",
    "        return best_params,best_auc,testauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data=pd.read_csv('./data_preprocessing/RT_SC_B_features.csv')\n",
    "financial_featrues=data\n",
    "X=financial_featrues.drop(['year','Stkcd','IndustryCode','label','EquityNatureID'],axis=1)\n",
    "for column in list(X.columns[X.isna().sum() > 0]):\n",
    "    mean_val = X[column].mean()\n",
    "    X[column].fillna(mean_val, inplace=True)\n",
    "\n",
    "X=X.values\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "X=minmax_scale(X)\n",
    "y=financial_featrues['label'].values\n",
    "###baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "params_dict=[]\n",
    "ne=list(range(5000,5010,250))\n",
    "max_depth=[None]\n",
    "min_leaf=[1]\n",
    "iters=itertools.product(ne,max_depth,min_leaf)\n",
    "for comb in iters:\n",
    "    params_dict.append({'n_estimators':comb[0],'max_depth':comb[1],'min_sample_leaf':comb[2]})\n",
    "print('total len of params comb:{}'.format(len(params_dict)))\n",
    "exp=experiment_protocal(params_dict=params_dict,model='RF',exp_name='baselian_RF_KG',comment='rpt RF baseline')\n",
    "re2=exp.simple_run(X,y)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF:\n",
    "best params:{'n_estimators': 2500, 'max_depth': None, 'min_sample_leaf': 1},bets auc on val set:0.7293685650514796\n",
    "test auc:0.727\n",
    "\n",
    "NN:\n",
    "best params[0.001, [1.0, 4.0], 0.01, 0.2],best epoch:1343,best val auc:0.6970900725703173,test auc: 0.694693088494896\n",
    "\n",
    "XGB:\n",
    "best params:{'max_depth': 10, 'lambda': 10},bets auc on val set:0.7142414860681114,test auc:0.6998954255890706\n",
    "\n",
    "KG+RF：\n",
    "best params:{'n_estimators': 3000, 'max_depth': None, 'min_sample_leaf': 1},bets auc on val set:0.7148270813833488\n",
    "test auc:0.7131447156343542\n",
    "\n",
    "KG+XGB\n",
    "{'max_depth': 10, 'lambda': 10, 'eval_metric': 'auc', 'eta': 0.03, 'gamma': 0.1}\n",
    "val auc:0.721 ;test auc:0.717"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "params_dict=[]\n",
    "max_depth=[10,20,6]\n",
    "gamma=[1,0.1]\n",
    "lambda1=[10]\n",
    "eta=[0.01,0.03,0.3,0.1]\n",
    "iters=itertools.product(max_depth,lambda1,gamma,eta)\n",
    "for comb in iters:\n",
    "    params_dict.append({'max_depth':comb[0],'lambda':comb[1],'eval_metric': 'auc','eta':comb[3],'gamma':comb[2]})\n",
    "result=[]\n",
    "train_X,test_X,train_y,test_y=train_test_split(X,y,stratify=y,test_size=0.2,random_state=5)\n",
    "train_X,val_X,train_y,val_y=train_test_split(train_X,train_y,stratify=train_y,random_state=5,test_size=0.25)\n",
    "Dtrain=xgboost.DMatrix(train_X,train_y)\n",
    "Dval=xgboost.DMatrix(val_X,val_y)\n",
    "Dtest=xgboost.DMatrix(test_X,test_y)\n",
    "for params in params_dict:\n",
    "    print(params)\n",
    "    model=xgboost.train(params,Dtrain,num_boost_round=2000,early_stopping_rounds=100,evals=[ (Dtrain, 'train'),(Dval, 'eval')],verbose_eval=200)\n",
    "    y_prob=model.predict(Dval)\n",
    "    auc=metrics.roc_auc_score(val_y,y_prob)\n",
    "    testauc=metrics.roc_auc_score(test_y,model.predict(Dtest))\n",
    "    result.append([auc,testauc])\n",
    "    print('val auc',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9366/9366 [05:07<00:00, 30.44it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "node_embedding=np.load('./data_preprocessing/dglke_result/ComplEx_KG4_1/KG4_ComplEx_entity.npy')\n",
    "node_dict=pd.read_csv('./data_preprocessing/dglke_dataset/KG4/entities.tsv',header=None,sep='\\t',quoting=csv.QUOTE_NONE,index_col=1).to_dict()[0]\n",
    "data=pd.read_csv('./data_preprocessing/RT_SC_B_features.csv')\n",
    "from tqdm import tqdm\n",
    "embed_dim=node_embedding.shape[1]\n",
    "new_col=list(range(0,embed_dim))\n",
    "for i in tqdm(data.index):\n",
    "    node=str(data.loc[i,'year'])+'.comp.'+ str(data.loc[i,'Stkcd'])\n",
    "    data.loc[i,new_col]=node_embedding[node_dict[node],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "from torch import tensor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "class nn_model(nn.Module):\n",
    "    def __init__(self, input_dim=343,embed_dim=1024):\n",
    "        super(nn_model, self).__init__()\n",
    "        self.mlp1=nn.Linear(input_dim,embed_dim)\n",
    "        self.mlp2=nn.Linear(embed_dim,embed_dim)\n",
    "        self.output=nn.Linear(embed_dim,2)\n",
    "        \n",
    "        self.layer_norm=nn.LayerNorm(normalized_shape= embed_dim)\n",
    "    def forward(self, feature,drop_out=0.0):\n",
    "        self.dropout = nn.Dropout(p=drop_out)\n",
    "        h1 = torch.sigmoid(self.dropout(self.mlp1(feature)))\n",
    "        h2 = torch.sigmoid(self.dropout(self.mlp2(h1)))\n",
    "        logits=self.output(self.layer_norm(h2))\n",
    "        return F.softmax(logits,dim=1)\n",
    "    \n",
    "class myNN:\n",
    "\n",
    "    def __init__(self,input_size,hidden_sizes,output_size=2,weight=[1.0,1.0],dropout=0.2,seed=555):\n",
    "        # Hyperparameters for our network\n",
    "        flag = torch.cuda.is_available()\n",
    "        #print(flag)\n",
    "        ngpu= 1\n",
    "        # Decide which device we want to run on\n",
    "        self.device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "        # Build a feed-forward network\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        self.input_size=input_size\n",
    "        self.drop=dropout\n",
    "        self.loss_func=nn.CrossEntropyLoss(tensor(weight,dtype=torch.float32).to(self.device))\n",
    "    def train(self,X,y,epoch=1000,lr=0.1,weight_decay=0.0,verbose=False,early_stopping=True):\n",
    "        self.model=nn_model(input_dim=self.input_size,embed_dim=1000).to(self.device)\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(),lr = lr,weight_decay=weight_decay)\n",
    "        if early_stopping:\n",
    "            X_train,X_val,y_train,y_val=train_test_split(X,y,stratify=y,test_size=0.3)\n",
    "            X_train,X_val,y_train,y_val=torch.tensor(X_train,dtype=torch.float32).to(self.device),torch.tensor(X_val,dtype=torch.float32).to(self.device),\\\n",
    "                torch.tensor(y_train,dtype=torch.long).to(self.device),torch.tensor(y_val,dtype=torch.long).to(self.device)\n",
    "        else:\n",
    "            X_train,y_train=X,y\n",
    "            X_train,y_train=torch.tensor(X_train,dtype=torch.float32).to(self.device),torch.tensor(y_train,dtype=torch.long).to(self.device)\n",
    "        if verbose==False:\n",
    "            printFlag=False \n",
    "        else:\n",
    "            printFlag=True\n",
    "            \n",
    "        notIncEpoch=0\n",
    "        maxAuc=0\n",
    "        train_auc_list=[]\n",
    "        val_auc_list=[]\n",
    "        for e in range(epoch):\n",
    "            self.model.train()\n",
    "            precition=self.model.forward(X_train,drop_out=self.drop)\n",
    "            precition=precition.to(torch.float32)\n",
    "            loss=self.loss_func(precition,y_train)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if early_stopping:\n",
    "                val_auc,_=self._test(X_val,y_val)\n",
    "                val_auc_list.append(val_auc)\n",
    "                if val_auc>=maxAuc:\n",
    "                    maxAuc=val_auc\n",
    "                    notIncEpoch=0\n",
    "                else:\n",
    "                    notIncEpoch+=1\n",
    "                if notIncEpoch>400:\n",
    "                    break\n",
    "            train_auc,_=self._test(X_train,y_train)\n",
    "            train_auc_list.append(train_auc)\n",
    "            if printFlag and e%verbose==0:\n",
    "                if early_stopping:\n",
    "                    print('========epoch{}========'.format(e))\n",
    "                    print('val auc:{}'.format(val_auc.item()))\n",
    "                print('training loss:{},train auc:{}'.format(loss.item(),train_auc.item()))\n",
    "        if early_stopping:\n",
    "            return train_auc_list,val_auc_list,np.argmax(val_auc_list)+1,max(val_auc_list)\n",
    "        return train_auc_list\n",
    "    def predict(self,X):\n",
    "        X=torch.tensor(X,dtype=torch.float32)\n",
    "        logits=self.model.forward(X)\n",
    "        return logits\n",
    "    def test(self,X,y):\n",
    "        X=torch.tensor(X,dtype=torch.float32).to(self.device)\n",
    "        y=torch.tensor(y,dtype=torch.long).to(self.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.model.forward(X)\n",
    "            predict_y = logits[:,1]\n",
    "            auc = roc_auc_score(y.cpu(),predict_y.cpu())\n",
    "        return auc, predict_y.cpu().numpy()\n",
    "\n",
    "    def _test(self,X,y):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.model.forward(X)\n",
    "            predict_y = logits[:,1]\n",
    "            auc = roc_auc_score(y.cpu().numpy(),predict_y.cpu().numpy())\n",
    "        return auc, predict_y.cpu().numpy()\n",
    "import pandas as pd\n",
    "#data=pd.read_csv('./new/RT_SC_B_features.csv')\n",
    "financial_featrues=data\n",
    "X=financial_featrues.drop(['year','Stkcd','IndustryCode','label','EquityNatureID'],axis=1)\n",
    "for column in list(X.columns[X.isna().sum() > 0]):\n",
    "    mean_val = X[column].mean()\n",
    "    X[column].fillna(mean_val, inplace=True)\n",
    "\n",
    "X=X.values\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "X=minmax_scale(X)\n",
    "y=financial_featrues['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [19:08<00:00, 31.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params[0.001, [1.0, 6.0], 0, 0.6],best epoch:665,best val auc:0.6840019209789878\n",
      "testing.....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,stratify=y,random_state=5)\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.25,stratify=y_train,random_state=5)\n",
    "parmas=[]\n",
    "leanring_rate=[0.01,0.001]\n",
    "l2=[0.001,0.01,0]\n",
    "weight=[[1.0,2.0],[1.0,6.0]]\n",
    "drop=[0.2,0.4,0.6]\n",
    "for l in l2:\n",
    "    for w in weight:\n",
    "        for lr in leanring_rate:\n",
    "            for d in drop:\n",
    "                parmas.append([lr,w,l,d])\n",
    "re=[]\n",
    "for p in tqdm(parmas):\n",
    "    model=myNN(input_size=493,hidden_sizes=[1000,1000],weight=p[1],dropout=p[3])\n",
    "    train_auc_list,val_auc_list,best_e,max_val_auc=model.train(X_train,y_train,epoch=2000,lr=p[0],weight_decay=p[2],verbose=False,early_stopping=True)\n",
    "    re.append([best_e,max_val_auc])\n",
    "best_id=np.argmax([i[1] for i in re])\n",
    "print('best params{},best epoch:{},best val auc:{}'.format(parmas[best_id],re[best_id][0],re[best_id][1]))\n",
    "print('testing.....')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========epoch0========\n",
      "val auc:0.52460957880757\n",
      "training loss:0.710503101348877,train auc:0.4925925925925926\n",
      "========epoch100========\n",
      "val auc:0.6312580040791158\n",
      "training loss:0.6923050284385681,train auc:0.6324169586621693\n",
      "========epoch200========\n",
      "val auc:0.6350050988948441\n",
      "training loss:0.6715267300605774,train auc:0.6420211546648329\n",
      "========epoch300========\n",
      "val auc:0.6473699188919982\n",
      "training loss:0.6496045589447021,train auc:0.6780910589339708\n",
      "========epoch400========\n",
      "val auc:0.6599392875776693\n",
      "training loss:0.6365774869918823,train auc:0.7156403161192433\n",
      "========epoch500========\n",
      "val auc:0.6694937864630272\n",
      "training loss:0.6215733289718628,train auc:0.7404724869284256\n",
      "========epoch600========\n",
      "val auc:0.6767448892472608\n",
      "training loss:0.6101200580596924,train auc:0.7531824780866926\n",
      "========epoch700========\n",
      "val auc:0.6802340748470332\n",
      "training loss:0.6070148348808289,train auc:0.7597668402266105\n",
      "========epoch800========\n",
      "val auc:0.6825078262106911\n",
      "training loss:0.5970363020896912,train auc:0.763046468218882\n",
      "========epoch900========\n",
      "val auc:0.6833111985960253\n",
      "training loss:0.6035490036010742,train auc:0.7659478665225791\n",
      "========epoch1000========\n",
      "val auc:0.6837039913674525\n",
      "training loss:0.5882045030593872,train auc:0.7682164259750467\n",
      "========epoch1100========\n",
      "val auc:0.6837351183417919\n",
      "training loss:0.5941178202629089,train auc:0.7697484472388687\n",
      "========epoch1200========\n",
      "val auc:0.6832489446473463\n",
      "training loss:0.5946177840232849,train auc:0.7711598497996965\n",
      "========epoch1300========\n",
      "val auc:0.6827983446378599\n",
      "training loss:0.5803133845329285,train auc:0.7721119734529696\n",
      "========epoch1400========\n",
      "val auc:0.6815651235592657\n",
      "training loss:0.5782193541526794,train auc:0.7730258920871947\n",
      "best epoch:0.6841812716406585,best val auc:1044\n"
     ]
    }
   ],
   "source": [
    "model=myNN(input_size=493,hidden_sizes=[1500,1500],weight=[1.0,6.0],dropout=0.6)\n",
    "train_auc_list,val_auc_list,best_e,max_val_auc=model.train(X_train,y_train,epoch=2000,lr=0.0001,weight_decay=0,verbose=100,early_stopping=True)\n",
    "print('best epoch:{},best val auc:{}'.format(best_e,max_val_auc))\n",
    "# testauc,_=model.test(X_test,y_test)\n",
    "# print('test auc:',testauc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[612, 0.69671061993075]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re[best_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
